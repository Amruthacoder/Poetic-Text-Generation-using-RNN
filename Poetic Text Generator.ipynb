{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f7b338-8371-410f-930a-786d074581a0",
   "metadata": {},
   "source": [
    "# Generating Texts with Recurrent Neural Networks in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f39dfc-fecf-461b-ace9-194ee0171c8e",
   "metadata": {},
   "source": [
    "Recurrent neural networks are very useful when it comes to the processing of sequential data like text. In this i am going to use LSTM neural networks (Long-Short-Term Memory) in order to teach the model to write texts like Shakespeare. we can train our model to guess the next letter based on the letters that came before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad4bb0-a7de-4066-83f2-e2e8e8519bae",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15143f50-ea9d-46d9-b853-8bd7c401a315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4254b40-d68d-4774-902d-91efd707fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "filepath= tf.keras.utils.get_file('shakespeare.txt',\n",
    "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88fe7ce-a5c2-4fea-85fa-37f94c564a13",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030d8f2-83a5-410c-9d99-aacf1250c9ea",
   "metadata": {},
   "source": [
    "The problem that we have right now with our data is that we are dealing with text. We cannot just train a neural network on letters or sentences. We need to convert all of these values into numerical data. So we have to come up with a system that allows us to convert the text into numbers, then predict specific numbers based on that data and then again convert the resulting numbers back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b12cf8-d6e2-4530-97fb-7639d808ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a9558-64d9-479a-b784-4c6c604253f2",
   "metadata": {},
   "source": [
    "In this case I immediately convert all of the text into lower-case so that we have fewer possible choices. Also I am not going to use the whole text file as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1277152-8d09-4251-b4d3-4346798e56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(text))\n",
    "\n",
    "char_to_index = dict((c,i) for i, c in enumerate(characters))\n",
    "index_to_char = dict((i,c) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35808f33-87ed-4f0b-af32-41729cb7b34e",
   "metadata": {},
   "source": [
    "Now we create a sorted set of all the unique characters that occur in the text. In a set no value appears more than once, so this is a good way to filter out the characters. After that we define two structures for converting the values. Both are dictionaries that enumerate the characters. In the first one, the characters are the keys and the indices are the values. In the second one it is the other way around. Now we can easily convert a character into a unique numerical representation and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11f6e03-3575-410e-9cb1-4a6b4432e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 40\n",
    "STEP_SIZE = 3\n",
    "\n",
    "sentences = []\n",
    "next_char = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986beee-b001-4786-b3ee-68baa3949a77",
   "metadata": {},
   "source": [
    "In this next step, we define how long a sequence shall be and also how many characters we will step further to start the next sentence. What we try to do here is to take sentences and then save the next letter as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5155d134-50bc-4619-859d-4413ebe0ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sentences.append(text[i: i + SEQ_LENGTH])\n",
    "    next_char.append(text[i + SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becf5c4-8d52-4049-8d45-0549bf508310",
   "metadata": {},
   "source": [
    "We iterate through the whole text and gather all sentences and their next character. This is the training data for our neural network. Now we just need to convert it into a numerical format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85b1e191-ca16-4f21-a8bf-efc977f223f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), SEQ_LENGTH,\n",
    "              len(characters)), dtype=np.bool_) \n",
    "y = np.zeros((len(sentences),\n",
    "              len(characters)), dtype=np.bool_)  \n",
    "\n",
    "for i, satz in enumerate(sentences):\n",
    "    for t, char in enumerate(satz):\n",
    "        x[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_char[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e9ad8-937e-46aa-b2fb-95073ec0eea5",
   "metadata": {},
   "source": [
    "We are creating two NumPy arrays full of zeros. The data type of those is bool, which stands for boolean. Wherever a character appears in a certain sentence at a certain position we will set it to a one or a True. We have one dimension for the sentences, one dimension for the positions of the characters within the sentences and one dimension to specify which character is at this position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a726564-9572-4ee8-ad3b-82a79885d417",
   "metadata": {},
   "source": [
    "## Building RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e089986-be20-47f3-bc43-e11de1e75df1",
   "metadata": {},
   "source": [
    "Now that the training data is prepared, we are going to build the neural network.The following specific tools that we are going to use is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3870dcbd-919c-46c2-ad4d-d9afa80d1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Activation, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc3893-ad13-4b63-abbc-59456b6e1ed1",
   "metadata": {},
   "source": [
    "We will use Sequential for our model, Activation, Dense and LSTM for our layers and RMSprop for optimization during the compilation of our model. LSTM stands for long-short-term memory and is a type of recurrent neural network layer. It might be called the memory of our model. This is crucial, since we are dealing with sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7027db7e-869a-4f4f-8b16-2fd37568a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(SEQ_LENGTH, len(characters))),\n",
    "    LSTM(128),\n",
    "    Dense(len(characters)),\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb049b-db36-4322-9406-63ec5eb70c14",
   "metadata": {},
   "source": [
    "The inputs immediately flow into our LSTM layer with 128 neurons. Our input shape is the length of a sentence times the amount of characters. The character which shall follow will be set to True or one. This layer is followed by a Dense hidden layer, which just increases complexity. In the end we use the Softmax activation function in order to make our results add up to one. This gives us the probability for each character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f446fe-3fbf-4c7d-8fb6-1c2bfd985188",
   "metadata": {},
   "source": [
    "Now we compile the model and train it with our training data that we prepared above. We choose a batch size of 256 (which you can change if you want) and four epochs. This means that our model is going to see the same data four times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4ff3a98-05fd-4112-8299-652ce0e5e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 38ms/step - loss: 2.2943\n",
      "Epoch 2/4\n",
      "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 44ms/step - loss: 1.6273\n",
      "Epoch 3/4\n",
      "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 45ms/step - loss: 1.5155\n",
      "Epoch 4/4\n",
      "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 46ms/step - loss: 1.4643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x308554bc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.01))\n",
    "\n",
    "model.fit(x, y, batch_size=256, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa39e69-b968-485e-bf75-f6366e06252b",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef64f2-5dab-47da-9bd5-71c10d8882b3",
   "metadata": {},
   "source": [
    "Now that the model is trained but it will only output the probabilities for the next character so i have added additional functions like helper function \"sample\" to make our script generate some reasonable text. The helper function i took as a reference from : https://keras.io/examples/lstm_text_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7098fce2-c89c-47c6-afab-ec82349c7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124687f1-fa1c-4a93-90cc-f0b8cd9f84ac",
   "metadata": {},
   "source": [
    "It basically just picks one of the characters from the output. As parameters it takes the result of the prediction and a temperature. This temperature indicates how risky the pick shall be. If we have a high temperature, we will pick one of the less likely characters. A low temperature will cause a conservative choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010925b8-558d-47ed-aeae-de0b970b6b68",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2697bb3c-bb4a-46ce-b3c5-7dce797178ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, temperature):\n",
    "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + SEQ_LENGTH]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "        x_predictions = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_predictions[0, t, char_to_index[char]] = 1\n",
    "\n",
    "        predictions = model.predict(x_predictions, verbose=0)[0]\n",
    "        next_index = sample(predictions,\n",
    "                                 temperature)\n",
    "        next_character = index_to_char[next_index]\n",
    "\n",
    "        generated += next_character\n",
    "        sentence = sentence[1:] + next_character\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e86039-c28b-4692-990a-f3652ee4e027",
   "metadata": {},
   "source": [
    "We basically choose a random starting position within the text because we need some starting text in order to predict the “next” character. So basically the first SEQ_LENGTH amount of characters will be copied from the original text. But we could just cut them off afterwards and we would end up with text that is completely generated by our neural network.\n",
    "\n",
    "So we choose some random starting text and then we run a for loop in the range of the length that we want. We can generate a text with 100 characters or one with 20,000. We then convert our sentence into the desired input format that we already talked about. \n",
    "\n",
    "The sentence is now an array with ones or Trues, wherever a character occurs. Then we use the predict method of our model, to predict the likelihoods of the next characters. Then we make use of our sample helper function. In this function we also have a temperature parameter, which we can pass to that helper function. Of course the result we get needs to be converted from the numerical format into a readable character. Once this is done, we add the character to our generated text and repeat the process, until we reach the desired length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88549ab0-a5bd-4a7b-a057-b6a73aec60a7",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62cfaaf8-4836-4c6d-90ff-1b0ac8b44c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutio, peace!\n",
      "thou talk'st of nothing.\n",
      "\n",
      "brutus:\n",
      "there is thee the man the wing to thee shall\n",
      "be thee than they make thee than they shall\n",
      "be the duke of thee than they are thee shall\n",
      "be the prince to the head thee than they love\n",
      "thee the son that thou not the brother with his cruell,\n",
      "that we will not do so him the war the hands than they a\n",
      "thou a kingdom; all of you allegiance:\n",
      "there is the rest to this morn to thee camillo\n",
      "england of the end there show i thank thee shall\n",
      "be so hath i look and think thee show i do not thee:\n",
      "what you do be the end men to thee to thee be\n",
      "the traitor to thee down do thee there is streef.\n",
      "\n",
      "menenius:\n",
      "i will not deeds thy son to have so head\n",
      "thee\n",
      "s before.\n",
      "\n",
      "first citizen:\n",
      "come, come, were i do but thee between hear\n",
      "thee seems as they she hath and show thee be\n",
      "comes thee there is not nothing have so look to thee.\n",
      "\n",
      "provost:\n",
      "what in the lords so thunk the prince and think there\n",
      "shall not duke of true to thee shall be a come\n",
      "and there there is thee be come thee than were\n",
      "they do the p\n",
      "orment'st me ere i come to hell!\n",
      "\n",
      "henry bolingbroke:\n",
      "he so would thee she were my lord.\n",
      "\n",
      "brutus:\n",
      "what hard, thank thou then, grace than the forther.\n",
      "\n",
      "brutus:\n",
      "believe thy partion,\n",
      "that they be a comes that i priiled a may\n",
      "there many of all thee man in her on thee.\n",
      "i say, thou shalt make thee than they seat,\n",
      "where is with other be our man b\n",
      "he white-upturned wondering eyes\n",
      "of morthantly is your house with his lord.\n",
      "\n",
      "warwick:\n",
      "with care, cliff, and i am but a preced me.\n",
      "\n",
      "first citizen:\n",
      "that well, shall to your ungently maid:\n",
      "but she hath wood are love thee weep i cet your life of head:\n",
      "than thou nest thou not leave me war than yet.\n",
      "\n",
      "capul:\n",
      "what she even and praised me sir, i s\n",
      " should be in hanging, if i\n",
      "should be have a lord that he are thy threats,\n",
      "thank actimes away than never seen them one,\n",
      "i have the rots servel art. what havise you.\n",
      "\n",
      "buckingham:\n",
      "then more, were there she she speaket\n",
      "and not paar of kill of him as have becessed.\n",
      "and you say, and enemies upon think is\n",
      "selled the cresel thee to the thing tha\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(300, 0.2))\n",
    "print(generate_text(300, 0.4))\n",
    "print(generate_text(300, 0.5))\n",
    "print(generate_text(300, 0.6))\n",
    "print(generate_text(300, 0.7))\n",
    "print(generate_text(300, 0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
